{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cae8e8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 1.864175,
     "end_time": "2022-08-05T14:26:00.612053",
     "exception": false,
     "start_time": "2022-08-05T14:25:58.747878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from types import SimpleNamespace  \n",
    "import yaml\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223971b",
   "metadata": {
    "papermill": {
     "duration": 5.98187,
     "end_time": "2022-08-05T14:26:06.629929",
     "exception": false,
     "start_time": "2022-08-05T14:26:00.648059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_CORES = mp.cpu_count()\n",
    "\n",
    "ID_SAMPLE = 0.01\n",
    "\n",
    "NUM_MODELS = [0,1,2]\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "if True:\n",
    "    data_folder = \"test\"\n",
    "    df = pd.read_csv(\"../input/pseudo-75-datasets-v1/old_competition_data.csv\")\n",
    "    CALC_SCORE = False\n",
    "else:\n",
    "    data_folder = \"train\"\n",
    "    df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\n",
    "    df.loc[df.discourse_id == \"56744a66949a\", \"discourse_text\"] = \"This whole thing is point less how they have us in here for two days im missing my education. We could have finished this in one day and had the rest of the week to get back on the track of learning. I've missed both days of weight lifting, algebra, and my world history that i do not want to fail again! If their are any people actually gonna sit down and take the time to read this then\\n\\nDO NOT DO THIS NEXT YEAR\\n\\n.\\n\\nThey are giving us cold lunches. ham and cheese and an apple, I am 16 years old and my body needs proper food. I wouldnt be complaining if they served actual breakfast. but because of Michelle Obama and her healthy diet rule they surve us 1 poptart in the moring. How does the school board expect us to last from 7:05-12:15 on a pop tart? then expect us to get A's, we are more focused on lunch than anything else. I am about done so if you have the time to read this even though this does not count. Bring PROPER_NAME a big Mac from mc donalds, SCHOOL_NAME, (idk area code but its in LOCATION_NAME)       \\xa0    \"\n",
    "\n",
    "    ids = df.essay_id.unique()\n",
    "    np.random.seed(1337)\n",
    "    val_ids = np.random.choice(ids, size=int(ID_SAMPLE*len(ids)), replace=False)\n",
    "    df = df[df.essay_id.isin(val_ids)]\n",
    "    df = df.reset_index(drop=True)\n",
    "    CALC_SCORE = True\n",
    "    \n",
    "print(CALC_SCORE)\n",
    "    \n",
    "df[\"discourse_type_essay\"] = df.groupby(\"essay_id\")[\"discourse_type\"].transform(lambda x: \" \".join(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a89e48",
   "metadata": {
    "papermill": {
     "duration": 0.044597,
     "end_time": "2022-08-05T14:26:06.743189",
     "exception": false,
     "start_time": "2022-08-05T14:26:06.698592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import collections\n",
    "\n",
    "class FeedbackDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, mode, cfg):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.architecture.cache_dir)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if self.tokenizer.sep_token is None:\n",
    "            self.tokenizer.sep_token = \" \"\n",
    "            \n",
    "        if hasattr(cfg.dataset, \"separator\") and len(cfg.dataset.separator):\n",
    "            self.cfg._tokenizer_sep_token = cfg.dataset.separator\n",
    "        else:\n",
    "            self.cfg._tokenizer_sep_token = self.tokenizer.sep_token\n",
    "                                                       \n",
    "        self.text = self.get_texts(self.df, self.cfg, self.tokenizer.sep_token)\n",
    "        \n",
    "        if self.cfg.tokenizer.lowercase:\n",
    "            self.df[\"essay_text\"] = self.df[\"essay_text\"].str.lower()\n",
    "            self.df[\"discourse_text\"] = self.df[\"discourse_text\"].str.lower()\n",
    "\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            grps = self.df.groupby(\"essay_id\", sort=False)\n",
    "            self.grp_texts = []\n",
    "            \n",
    "            s = 0\n",
    "\n",
    "            for grp in grps.groups:\n",
    "                g = grps.get_group(grp)\n",
    "                t = g.essay_text.values[0]\n",
    "                \n",
    "                end = 0\n",
    "                for j in range(len(g)):\n",
    "\n",
    "                    d = g.discourse_text.values[j]\n",
    "                    start = t[end:].find(d.strip()) \n",
    "                    if start == -1:\n",
    "                        print(\"ERROR\")\n",
    "                    \n",
    "                    start = start + end\n",
    "                    end = start + len(d.strip())\n",
    "                    if self.cfg.architecture.aux_type:\n",
    "                        t = t[:start] + f\" [START] \" + t[start:end] + \" [END] \" + t[end:] \n",
    "                    elif self.cfg.architecture.use_type:\n",
    "                        t = t[:start] + f\" [START_{g.discourse_type.values[j]}]  \" + t[start:end] + f\" [END_{g.discourse_type.values[j]}] \" + t[end:] \n",
    "                    else:\n",
    "                        t = t[:start] + f\" [START] {g.discourse_type.values[j]} \" + t[start:end] + \" [END] \" + t[end:] \n",
    "\n",
    "                if hasattr(self.cfg.dataset, \"add_group_types\") and self.cfg.dataset.add_group_types:\n",
    "                    t = \" \".join(g.discourse_type.values) + f\" {self.cfg._tokenizer_sep_token} \" + t\n",
    "                        \n",
    "                self.grp_texts.append(t)\n",
    "\n",
    "                s += len(g)\n",
    "\n",
    "            if self.cfg.dataset.group_discourse:\n",
    "                \n",
    "                self.cfg._tokenizer_start_token_id = []\n",
    "                self.cfg._tokenizer_end_token_id = []\n",
    "\n",
    "                if self.cfg.architecture.use_type:\n",
    "                    for type in sorted(self.df.discourse_type.unique()):\n",
    "                        self.tokenizer.add_tokens([f\"[START_{type}]\"], special_tokens=True)\n",
    "                        self.cfg._tokenizer_start_token_id.append(self.tokenizer.encode(f\"[START_{type}]\")[1])\n",
    "                    \n",
    "                    for type in sorted(self.df.discourse_type.unique()):\n",
    "                        self.tokenizer.add_tokens([f\"[END_{type}]\"], special_tokens=True)\n",
    "                        self.cfg._tokenizer_end_token_id.append(self.tokenizer.encode(f\"[END_{type}]\")[1])\n",
    "\n",
    "                else:\n",
    "                    self.tokenizer.add_tokens([\"[START]\", \"[END]\"], special_tokens=True)\n",
    "                    self.cfg._tokenizer_start_token_id.append(self.tokenizer.encode(f\"[START]\")[1])\n",
    "                    self.cfg._tokenizer_end_token_id.append(self.tokenizer.encode(f\"[END]]\")[1])\n",
    "\n",
    "                print(self.cfg._tokenizer_start_token_id)\n",
    "                print(self.cfg._tokenizer_end_token_id)\n",
    "\n",
    "            if hasattr(self.cfg.tokenizer, \"add_newline_token\") and self.cfg.tokenizer.add_newline_token:\n",
    "                self.tokenizer.add_tokens([f\"\\n\"], special_tokens=True)\n",
    "\n",
    "            self.cfg._tokenizer_size = len(self.tokenizer)\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            return len(self.grp_texts)\n",
    "        else:\n",
    "            return len(self.df)\n",
    "        \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        elem = batch[0]\n",
    "\n",
    "        ret = {}\n",
    "        for key in elem:\n",
    "            if key in {\"target\", \"weight\"}:\n",
    "                ret[key] = [d[key].float() for d in batch]\n",
    "            elif key in {\"target_aux\"}:\n",
    "\n",
    "                ret[key] = [d[key].float() for d in batch]\n",
    "            else:\n",
    "                ret[key] = torch.stack([d[key] for d in batch], 0)\n",
    "        return ret\n",
    "            \n",
    "    def batch_to_device(batch, device):\n",
    "\n",
    "        if isinstance(batch, torch.Tensor):\n",
    "            return batch.to(device)\n",
    "        elif isinstance(batch, collections.abc.Mapping):\n",
    "            return {\n",
    "                key: FeedbackDataset.batch_to_device(value, device)\n",
    "                for key, value in batch.items()\n",
    "            }\n",
    "        elif isinstance(batch, collections.abc.Sequence):\n",
    "            return [FeedbackDataset.batch_to_device(value, device) for value in batch]\n",
    "        else:\n",
    "            raise ValueError(f\"Can not move {type(batch)} to device.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _lowercase(sample):\n",
    "        if isinstance(sample, str):\n",
    "            return sample.lower()\n",
    "        elif isinstance(sample, Iterable):\n",
    "            return [x.lower() for x in sample]\n",
    "    \n",
    "    def get_texts(cls, df, cfg, separator):\n",
    "        if separator is None:\n",
    "            if hasattr(cfg.dataset, \"separator\") and len(cfg.dataset.separator):\n",
    "                separator = cfg.dataset.separator\n",
    "            else:\n",
    "                separator = getattr(cfg, \"_tokenizer_sep_token\", \"<SEPARATOR>\")\n",
    "\n",
    "        lowercase = hasattr(cfg, \"tokenizer\") and cfg.tokenizer.lowercase\n",
    "        if isinstance(cfg.dataset.text_column, str):\n",
    "            texts = df[cfg.dataset.text_column].astype(str)\n",
    "            if lowercase:\n",
    "                texts = texts.apply(cls._lowercase)\n",
    "            texts = texts.values\n",
    "        else:\n",
    "            columns = list(cfg.dataset.text_column)\n",
    "            join_str = f\" {separator} \"\n",
    "            texts = df[columns].astype(str)\n",
    "            if lowercase:\n",
    "                texts = texts.apply(cls._lowercase)\n",
    "            texts = texts.apply(lambda x: join_str.join(x), axis=1).values\n",
    "\n",
    "        return texts\n",
    "        \n",
    "    def _read_data(self, idx, sample):\n",
    "\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            text = self.grp_texts[idx]\n",
    "        else:\n",
    "            text = self.text[idx]\n",
    "\n",
    "        if idx == 0:\n",
    "            print(text)\n",
    "            \n",
    "        sample.update(self.encode(text))\n",
    "        return sample\n",
    "    \n",
    "    def encode(self, text):\n",
    "        sample = dict()\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.cfg.tokenizer.max_length,\n",
    "        )\n",
    "        sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n",
    "        sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict()\n",
    "            \n",
    "        sample = self._read_data(idx=idx, sample=sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb55595",
   "metadata": {
    "papermill": {
     "duration": 0.034646,
     "end_time": "2022-08-05T14:26:06.786032",
     "exception": false,
     "start_time": "2022-08-05T14:26:06.751386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "class NLPAllclsTokenPooling(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, cfg):\n",
    "        super(NLPAllclsTokenPooling, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.feat_mult = 1\n",
    "        if cfg.dataset.group_discourse:\n",
    "            self.feat_mult = 3\n",
    "\n",
    "    def forward(self, x, attention_mask, input_ids, cfg):\n",
    "\n",
    "        if not cfg.dataset.group_discourse:\n",
    "            input_ids_expanded = input_ids.clone().unsqueeze(-1).expand(x.shape)\n",
    "            attention_mask_expanded = torch.zeros_like(input_ids_expanded)\n",
    "\n",
    "            attention_mask_expanded[(input_ids_expanded == cfg._tokenizer_cls_token_id) | (input_ids_expanded == cfg._tokenizer_sep_token_id)] = 1\n",
    "\n",
    "            sum_features = (x * attention_mask_expanded).sum(self.dim)\n",
    "            ret = sum_features / attention_mask_expanded.sum(self.dim).clip(min=1e-8)\n",
    "\n",
    "        else:\n",
    "            ret = []\n",
    "\n",
    "            for j in range(x.shape[0]):\n",
    "\n",
    "\n",
    "                idx0 = torch.where((input_ids[j] >= min(cfg._tokenizer_start_token_id)) & (input_ids[j] <= max(cfg._tokenizer_start_token_id)))[0]\n",
    "                idx1 = torch.where((input_ids[j] >= min(cfg._tokenizer_end_token_id)) & (input_ids[j] <= max(cfg._tokenizer_end_token_id)))[0]\n",
    "\n",
    "                xx = []\n",
    "                for jj in range(len(idx0)):\n",
    "                    xx0 = x[j, idx0[jj]]\n",
    "                    xx1 = x[j, idx1[jj]]\n",
    "                    xx2 = x[j, idx0[jj]+1:idx1[jj]].mean(dim=0)\n",
    "                    xxx = torch.cat([xx0, xx1, xx2]).unsqueeze(0)\n",
    "                    xx.append(xxx)\n",
    "                xx = torch.cat(xx)\n",
    "                ret.append(xx)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim, cfg, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, x, attention_mask, input_ids, cfg):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n",
    "        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret\n",
    "    \n",
    "class NLPPoolings:\n",
    "    _poolings = {\n",
    "        \"All [CLS] token\": NLPAllclsTokenPooling,\n",
    "        \"GeM\": GeMText\n",
    "    }\n",
    "    @classmethod\n",
    "    def get(cls, name):\n",
    "        return cls._poolings.get(name)\n",
    "\n",
    "class FeedbackModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        super(FeedbackModel, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.n_classes = 3\n",
    "        config = AutoConfig.from_pretrained(cfg.architecture.cache_dir)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "    \n",
    "        self.backbone.pooler = None\n",
    "        \n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            self.backbone.resize_token_embeddings(cfg._tokenizer_size)\n",
    "        \n",
    "        self.pooling = NLPPoolings.get(self.cfg.architecture.pool)\n",
    "        self.pooling = self.pooling(dim=1, cfg=cfg)  # init pooling and pool over token dimension\n",
    "        \n",
    "        self.head = nn.Linear(self.backbone.config.hidden_size*self.pooling.feat_mult, self.n_classes)\n",
    "\n",
    "    def get_features(self, batch):\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "\n",
    "        x = self.backbone(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        x = self.pooling(x, attention_mask, input_ids, cfg=self.cfg)\n",
    "\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            x = torch.cat(x)\n",
    "        \n",
    "        if self.cfg.architecture.dropout > 0.0:\n",
    "            x = F.dropout(x, p=self.cfg.architecture.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, batch, calculate_loss=False):\n",
    "        \n",
    "        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max())\n",
    "        idx += 1\n",
    "        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n",
    "        \n",
    "        x = self.get_features(batch)\n",
    "                \n",
    "        logits = self.head(x)\n",
    "        outputs = {}\n",
    "\n",
    "        outputs[\"logits\"] = logits\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670faec",
   "metadata": {
    "papermill": {
     "duration": 0.853554,
     "end_time": "2022-08-05T14:26:07.647988",
     "exception": false,
     "start_time": "2022-08-05T14:26:06.794434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, PreTrainedModel\n",
    "\n",
    "def create_nlp_backbone(\n",
    "    cfg, model_class=AutoModel, remove_pooling_layer=False\n",
    "):\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        cfg[\"backbone\"], cache_dir=cfg[\"cache_dir\"]\n",
    "    )\n",
    "\n",
    "    kwargs = dict(add_pooling_layer=False) if remove_pooling_layer else dict()\n",
    "    \n",
    "    try:\n",
    "        backbone = model_class.from_config(config, **kwargs)\n",
    "    except TypeError:\n",
    "        backbone = model_class.from_config(config)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "def glorot_uniform(parameter):\n",
    "    nn.init.xavier_uniform_(parameter.data, gain=1.0)\n",
    "\n",
    "\n",
    "class NBMEHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NBMEHead, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(input_dim, output_dim)\n",
    "        glorot_uniform(self.classifier.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is B x S x C\n",
    "        logits1 = self.classifier(self.dropout1(x))\n",
    "        logits2 = self.classifier(self.dropout2(x))\n",
    "        logits3 = self.classifier(self.dropout3(x))\n",
    "        logits4 = self.classifier(self.dropout4(x))\n",
    "        logits5 = self.classifier(self.dropout5(x))\n",
    "\n",
    "        logits = ((logits1 + logits2 + logits3 + logits4 + logits5) / 5)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ModelYauhen(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ModelYauhen, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.n_classes = 3\n",
    "        self.backbone = create_nlp_backbone(\n",
    "            self.cfg,\n",
    "            model_class=AutoModel,\n",
    "            remove_pooling_layer=False,\n",
    "        )\n",
    "        self.head = nn.Linear(self.backbone.config.hidden_size, 3)\n",
    "        if self.cfg[\"add_wide_dropout\"]:\n",
    "            self.token_type_head = NBMEHead(self.backbone.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, batch, calculate_loss=True):\n",
    "        outputs = {}\n",
    "        \n",
    "        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max()) \n",
    "        idx += 1\n",
    "        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n",
    "        batch[\"word_start_mask\"] = batch[\"word_start_mask\"][:, :idx]\n",
    "        batch[\"word_ids\"] = batch[\"word_ids\"][:, :idx]\n",
    "\n",
    "        outputs[\"word_start_mask\"] = batch[\"word_start_mask\"]\n",
    "\n",
    "        x = self.backbone(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        ).last_hidden_state\n",
    "\n",
    "        for obs_id in range(x.size()[0]):\n",
    "            for w_id in range(int(torch.max(batch[\"word_ids\"][obs_id]).item()) + 1):\n",
    "                chunk_mask = batch[\"word_ids\"][obs_id] == w_id\n",
    "                chunk_logits = x[obs_id] * chunk_mask.unsqueeze(-1)\n",
    "                chunk_logits = chunk_logits.sum(dim=0) / chunk_mask.sum()\n",
    "                x[obs_id][chunk_mask] = chunk_logits\n",
    "\n",
    "        if self.cfg[\"add_wide_dropout\"]:\n",
    "            logits = self.token_type_head(x)\n",
    "        else:\n",
    "            logits = self.head(x)\n",
    "        outputs[\"logits\"] = logits      \n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class FeedbackDatasetYauhen(Dataset):\n",
    "    @staticmethod\n",
    "    def _lowercase(sample):\n",
    "        if isinstance(sample, str):\n",
    "            return sample.lower()\n",
    "        elif isinstance(sample, Iterable):\n",
    "            return [x.lower() for x in sample]\n",
    "\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.cfg[\"backbone\"],\n",
    "            add_prefix_space=True,\n",
    "            use_fast=True,\n",
    "            cache_dir=self.cfg[\"cache_dir\"],\n",
    "        )\n",
    "\n",
    "        self.text = self.get_texts(self.df, self.cfg, self.tokenizer.sep_token)\n",
    "        self.labels = self.df[\"tokens\"].values\n",
    "\n",
    "    @classmethod\n",
    "    def get_texts(cls, df, cfg, separator=None):\n",
    "        texts = df[cfg[\"text_column\"]].values\n",
    "\n",
    "        if cfg[\"lowercase\"]:\n",
    "            texts = [cls._lowercase(x) for x in texts]\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict()\n",
    "            \n",
    "        text = self.text[idx]\n",
    "        \n",
    "        if \"deberta-v3\" in self.cfg[\"backbone\"]:\n",
    "            text = [x.replace(\"\\n\", \"[NL_HYDRO]\") for x in list(text)]\n",
    "            text = [x if not x.isspace() else \"[SP_HYDRO]\" * len(x) for x in text]\n",
    "            tokenizer_input = [text]\n",
    "            raise ValueError(f\"BES {text}\")\n",
    "        else:\n",
    "            if \"add_types\" in self.cfg and self.cfg[\"add_types\"]:\n",
    "                tokenizer_input = [x if x_idx > 0 else x + self.tokenizer.sep_token for x_idx, x in enumerate(list(text))]\n",
    "            else:\n",
    "                tokenizer_input = [list(text)]\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            tokenizer_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "\n",
    "        sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n",
    "        sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n",
    "\n",
    "        word_ids = encodings.word_ids(0)\n",
    "        word_ids = [-1 if x is None else x for x in word_ids]\n",
    "        sample[\"word_ids\"] = torch.tensor(word_ids)\n",
    "\n",
    "        word_start_mask = []\n",
    "        lab_idx = -1\n",
    "        for i, word in enumerate(word_ids):\n",
    "            word_start = word > -1 and (i == 0 or word_ids[i - 1] != word)\n",
    "            if word_start:\n",
    "                lab_idx += 1\n",
    "                if self.labels[idx][lab_idx] != 1:\n",
    "                    word_start_mask.append(True)\n",
    "                    continue\n",
    "\n",
    "            word_start_mask.append(False)\n",
    "\n",
    "        sample[\"word_start_mask\"] = torch.tensor(word_start_mask)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad491637",
   "metadata": {
    "papermill": {
     "duration": 0.087441,
     "end_time": "2022-08-05T14:26:07.744314",
     "exception": false,
     "start_time": "2022-08-05T14:26:07.656873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f56e66",
   "metadata": {
    "papermill": {
     "duration": 12.412001,
     "end_time": "2022-08-05T14:26:20.164753",
     "exception": false,
     "start_time": "2022-08-05T14:26:07.752752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_obs = []\n",
    "\n",
    "for name, gr in tqdm(test.groupby(\"essay_id\", sort=False)):\n",
    "    essay_text_start_end = gr.essay_text.values[0]\n",
    "    token_labels = []\n",
    "    token_obs = []\n",
    "    end_pos = 0\n",
    "    \n",
    "    for idx, row in gr.reset_index(drop=True).iterrows():\n",
    "        target_text = row[\"discourse_type\"] + \" \" + row[\"discourse_text\"].strip()\n",
    "        \n",
    "        essay_text_start_end = essay_text_start_end[:end_pos] + essay_text_start_end[end_pos:].replace(row[\"discourse_text\"].strip(), target_text, 1)\n",
    "        \n",
    "        start_pos = essay_text_start_end[end_pos:].find(target_text)\n",
    "        if start_pos == -1:\n",
    "            raise ValueError()\n",
    "        start_pos += end_pos\n",
    "        \n",
    "        if idx == 0 and start_pos > 0:\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[:start_pos])\n",
    "        \n",
    "        if start_pos > end_pos and end_pos > 0:\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[end_pos:start_pos])\n",
    "  \n",
    "        end_pos = start_pos + len(target_text)\n",
    "        token_labels.append(0)\n",
    "        token_obs.append(essay_text_start_end[start_pos: end_pos])\n",
    "            \n",
    "        if idx == len(gr) - 1 and end_pos < len(essay_text_start_end):\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[end_pos:])\n",
    "            \n",
    "    if len(token_labels) != len(token_obs):\n",
    "        raise ValueError()\n",
    "            \n",
    "    all_obs.append((name, token_labels, token_obs))\n",
    "\n",
    "tt = pd.DataFrame(all_obs, columns=[\"essay_id\", \"tokens\", \"essay_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14e46a",
   "metadata": {
    "papermill": {
     "duration": 0.060025,
     "end_time": "2022-08-05T14:26:20.260695",
     "exception": false,
     "start_time": "2022-08-05T14:26:20.200670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_predictions_philipp(exp_name, df, BS=1, num_models=NUM_MODELS):\n",
    "    \n",
    "    cfg = yaml.safe_load(open(f\"../input/{exp_name}/cfg-fold0.yaml\").read())\n",
    "    for k,v in cfg.items():\n",
    "        if type(v) == dict:\n",
    "            cfg[k] = SimpleNamespace(**v)\n",
    "    cfg = SimpleNamespace(**cfg)\n",
    "\n",
    "    if cfg.architecture.backbone == 'microsoft/deberta-v3-large':\n",
    "        cfg.architecture.cache_dir = \"../input/deberta-v3-large/\"\n",
    "\n",
    "    ds = FeedbackDataset(df.iloc[:], mode=\"test\", cfg=cfg)\n",
    "    \n",
    "    preds_all = []\n",
    "    for fold in NUM_MODELS:\n",
    "        print(f\"running model {fold}\")\n",
    "        \n",
    "        model = FeedbackModel(cfg).to(\"cuda\").eval()\n",
    "    \n",
    "        d = torch.load(f\"../input/{exp_name}/checkpoint-fold{fold}.pth\", map_location=\"cpu\")\n",
    "\n",
    "        model_weights = d[\"model\"]\n",
    "        model_weights = {k.replace(\"module.\", \"\"): v for k, v in model_weights.items()}\n",
    "        \n",
    "        for k in list(model_weights.keys()):\n",
    "            if \"aux\" in k or \"loss_fn\" in k:\n",
    "                del model_weights[k]\n",
    "\n",
    "        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n",
    "        \n",
    "        del d\n",
    "        del model_weights\n",
    "        gc.collect() \n",
    "    \n",
    "        batch_size = BS\n",
    "        dl = DataLoader(ds, shuffle=False, batch_size = batch_size, num_workers = N_CORES)\n",
    "\n",
    "        with torch.no_grad():    \n",
    "            preds = []\n",
    "            for batch in tqdm(dl):\n",
    "\n",
    "                batch = FeedbackDataset.batch_to_device(batch, \"cuda\")\n",
    "                out = model(batch)\n",
    "                preds.append(out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        preds_all.append(np.concatenate(preds, axis=0))\n",
    "        \n",
    "        del model\n",
    "        del dl\n",
    "        gc.collect()\n",
    "        \n",
    "    del ds\n",
    "    \n",
    "    \n",
    "    preds = np.mean(preds_all, axis=0)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def run_predictions_yauhen(all_cfgs, df, num_models=NUM_MODELS):\n",
    "    ds = FeedbackDatasetYauhen(df=df, cfg=all_cfgs[0])\n",
    "        \n",
    "    preds_all = []\n",
    "\n",
    "    for fold_idx in NUM_MODELS:\n",
    "        params = all_cfgs[fold_idx]\n",
    "\n",
    "        model = ModelYauhen(params).to(\"cuda\").eval()\n",
    "\n",
    "        d = torch.load(params[\"path\"], map_location=\"cpu\")\n",
    "\n",
    "        model_weights = d[\"model\"]\n",
    "        model_weights = {k.replace(\"module.\", \"\"): v for k, v in model_weights.items()}\n",
    "        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n",
    "        \n",
    "        del d\n",
    "        del model_weights\n",
    "        gc.collect() \n",
    "    \n",
    "        batch_size = 1\n",
    "        dl = DataLoader(ds, shuffle=False, batch_size = batch_size, num_workers = N_CORES)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds = []\n",
    "            for batch in tqdm(dl):\n",
    "                texts = {\n",
    "                key: value.to(\"cuda\")\n",
    "                for key, value in batch.items()\n",
    "            }\n",
    "                output = model.forward(texts, calculate_loss=False)\n",
    "\n",
    "                val = (\n",
    "                        torch.softmax(output[\"logits\"][output[\"word_start_mask\"]], dim=1).detach().cpu().numpy()\n",
    "                    )\n",
    "\n",
    "                preds.append(val)\n",
    "\n",
    "        preds_all.append(np.concatenate(preds, axis=0))\n",
    "        \n",
    "        del model\n",
    "        del dl\n",
    "        gc.collect()\n",
    "        \n",
    "    del ds\n",
    "    \n",
    "    preds = np.mean(preds_all, axis=0)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af798e5f",
   "metadata": {
    "papermill": {
     "duration": 0.031973,
     "end_time": "2022-08-05T14:26:20.314432",
     "exception": false,
     "start_time": "2022-08-05T14:26:20.282459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fba4cb",
   "metadata": {
    "papermill": {
     "duration": 4347.12087,
     "end_time": "2022-08-05T15:38:47.464022",
     "exception": false,
     "start_time": "2022-08-05T14:26:20.343152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/fearless-ara-ff/checkpoint-fold0.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/fearless-ara-ff/checkpoint-fold1.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/fearless-ara-ff/checkpoint-fold2.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "preds.append(run_predictions_yauhen(all_cfgs, tt))\n",
    "weights.append(1.0784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430ca42",
   "metadata": {
    "papermill": {
     "duration": 4376.85076,
     "end_time": "2022-08-05T16:51:48.108424",
     "exception": false,
     "start_time": "2022-08-05T15:38:51.257664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/meteoric-bettong-v2-ff/checkpoint-fold0.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/meteoric-bettong-v2-ff/checkpoint-fold1.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/meteoric-bettong-v2-ff/checkpoint-fold2.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "preds.append(run_predictions_yauhen(all_cfgs, tt))\n",
    "weights.append(1.5709)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848297c",
   "metadata": {
    "papermill": {
     "duration": 6838.744,
     "end_time": "2022-08-05T18:45:54.835364",
     "exception": false,
     "start_time": "2022-08-05T16:51:56.091364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(run_predictions_philipp(\"valiant-degu-ff-2\", df, BS=8))\n",
    "weights.append(0.6330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff067058",
   "metadata": {
    "papermill": {
     "duration": 5004.899064,
     "end_time": "2022-08-05T20:09:24.460591",
     "exception": false,
     "start_time": "2022-08-05T18:45:59.561527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(run_predictions_philipp(\"axiomatic-vulture-ff\", df))\n",
    "weights.append(0.8380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152b8c1",
   "metadata": {
    "papermill": {
     "duration": 4897.974459,
     "end_time": "2022-08-05T21:31:08.457790",
     "exception": false,
     "start_time": "2022-08-05T20:09:30.483331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(run_predictions_philipp(\"dreamy-bird-ff\", df))\n",
    "weights.append(1.0189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb374993",
   "metadata": {
    "papermill": {
     "duration": 4900.665611,
     "end_time": "2022-08-05T22:52:56.709713",
     "exception": false,
     "start_time": "2022-08-05T21:31:16.044102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(run_predictions_philipp(\"smart-bumblebee-ff\", df))\n",
    "weights.append(0.9697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fae21c",
   "metadata": {
    "papermill": {
     "duration": 8.894185,
     "end_time": "2022-08-05T22:53:15.081609",
     "exception": false,
     "start_time": "2022-08-05T22:53:06.187424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = np.average(preds, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d4f40",
   "metadata": {
    "papermill": {
     "duration": 9.099635,
     "end_time": "2022-08-05T22:55:40.822817",
     "exception": false,
     "start_time": "2022-08-05T22:55:31.723182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Adequate\"] = preds[:, 0]\n",
    "df[\"Effective\"] = preds[:, 1]\n",
    "df[\"Ineffective\"] = preds[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed9e52",
   "metadata": {
    "papermill": {
     "duration": 9.030631,
     "end_time": "2022-08-05T22:55:58.626929",
     "exception": false,
     "start_time": "2022-08-05T22:55:49.596298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CALC_SCORE:\n",
    "    from sklearn.metrics import log_loss\n",
    "    \n",
    "    label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n",
    "    \n",
    "    y = np.zeros_like(preds)\n",
    "    \n",
    "    for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n",
    "        y[ii,jj] = 1\n",
    "        \n",
    "    print(log_loss(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d5418",
   "metadata": {
    "papermill": {
     "duration": 9.642926,
     "end_time": "2022-08-05T22:58:42.586291",
     "exception": false,
     "start_time": "2022-08-05T22:58:32.943365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['discourse_id', 'Ineffective', 'Adequate', 'Effective']].to_csv(\"pseudo_104_ff_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ba67e",
   "metadata": {
    "papermill": {
     "duration": 9.058269,
     "end_time": "2022-08-05T22:59:37.136999",
     "exception": false,
     "start_time": "2022-08-05T22:59:28.078730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30873.021346,
   "end_time": "2022-08-05T22:59:47.715761",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-05T14:25:14.694415",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
