{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e02c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.183636,
     "end_time": "2022-08-22T20:15:34.265051",
     "exception": false,
     "start_time": "2022-08-22T20:15:32.081415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from types import SimpleNamespace  \n",
    "import yaml\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2831eb",
   "metadata": {
    "papermill": {
     "duration": 0.306176,
     "end_time": "2022-08-22T20:15:34.624039",
     "exception": false,
     "start_time": "2022-08-22T20:15:34.317863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_CORES = mp.cpu_count()\n",
    "\n",
    "ID_SAMPLE = 0.01\n",
    "\n",
    "NUM_MODELS = 3\n",
    "\n",
    "if True:\n",
    "    data_folder = \"test\"\n",
    "    df = pd.read_csv(\"../input/pseudo-75-datasets-v1/old_competition_data.csv\")\n",
    "    CALC_SCORE = False\n",
    "else:\n",
    "    data_folder = \"train\"\n",
    "    df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\n",
    "    df.loc[df.discourse_id == \"56744a66949a\", \"discourse_text\"] = \"This whole thing is point less how they have us in here for two days im missing my education. We could have finished this in one day and had the rest of the week to get back on the track of learning. I've missed both days of weight lifting, algebra, and my world history that i do not want to fail again! If their are any people actually gonna sit down and take the time to read this then\\n\\nDO NOT DO THIS NEXT YEAR\\n\\n.\\n\\nThey are giving us cold lunches. ham and cheese and an apple, I am 16 years old and my body needs proper food. I wouldnt be complaining if they served actual breakfast. but because of Michelle Obama and her healthy diet rule they surve us 1 poptart in the moring. How does the school board expect us to last from 7:05-12:15 on a pop tart? then expect us to get A's, we are more focused on lunch than anything else. I am about done so if you have the time to read this even though this does not count. Bring PROPER_NAME a big Mac from mc donalds, SCHOOL_NAME, (idk area code but its in LOCATION_NAME)       \\xa0    \"\n",
    "\n",
    "    ids = df.essay_id.unique()\n",
    "    np.random.seed(1337)\n",
    "    val_ids = np.random.choice(ids, size=int(ID_SAMPLE*len(ids)), replace=False)\n",
    "    df = df[df.essay_id.isin(val_ids)]\n",
    "    df = df.reset_index(drop=True)\n",
    "    CALC_SCORE = True\n",
    "    \n",
    "print(CALC_SCORE)\n",
    "    \n",
    "df[\"discourse_type_essay\"] = df.groupby(\"essay_id\")[\"discourse_type\"].transform(lambda x: \" \".join(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d04d01",
   "metadata": {
    "papermill": {
     "duration": 13.630427,
     "end_time": "2022-08-22T20:15:48.307866",
     "exception": false,
     "start_time": "2022-08-22T20:15:34.677439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df[\"count\"] = df[\"essay_text\"].apply(lambda x: len(x))\n",
    "df[\"orig_order\"] = range(len(df))\n",
    "df = df.sort_values([\"count\", \"essay_id\", \"orig_order\"], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b12804",
   "metadata": {
    "papermill": {
     "duration": 0.059197,
     "end_time": "2022-08-22T20:15:48.388361",
     "exception": false,
     "start_time": "2022-08-22T20:15:48.329164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import collections\n",
    "\n",
    "class FeedbackDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, mode, cfg):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.architecture.cache_dir)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if self.tokenizer.sep_token is None:\n",
    "            self.tokenizer.sep_token = \" \"\n",
    "            \n",
    "        if hasattr(cfg.dataset, \"separator\") and len(cfg.dataset.separator):\n",
    "            self.cfg._tokenizer_sep_token = cfg.dataset.separator\n",
    "        else:\n",
    "            self.cfg._tokenizer_sep_token = self.tokenizer.sep_token\n",
    "                                                       \n",
    "        self.text = self.get_texts(self.df, self.cfg, self.tokenizer.sep_token)\n",
    "        \n",
    "        if self.cfg.tokenizer.lowercase:\n",
    "            self.df[\"essay_text\"] = self.df[\"essay_text\"].str.lower()\n",
    "            self.df[\"discourse_text\"] = self.df[\"discourse_text\"].str.lower()\n",
    "\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            grps = self.df.groupby(\"essay_id\", sort=False)\n",
    "            self.grp_texts = []\n",
    "            \n",
    "            s = 0\n",
    "\n",
    "            for grp in grps.groups:\n",
    "                g = grps.get_group(grp)\n",
    "                t = g.essay_text.values[0]\n",
    "                \n",
    "                end = 0\n",
    "                for j in range(len(g)):\n",
    "\n",
    "                    d = g.discourse_text.values[j]\n",
    "                    start = t[end:].find(d.strip()) \n",
    "                    if start == -1:\n",
    "                        print(\"ERROR\")\n",
    "                    \n",
    "                    start = start + end\n",
    "                    end = start + len(d.strip())\n",
    "                    if self.cfg.architecture.aux_type:\n",
    "                        t = t[:start] + f\" [START] \" + t[start:end] + \" [END] \" + t[end:] \n",
    "                    elif self.cfg.architecture.use_type:\n",
    "                        t = t[:start] + f\" [START_{g.discourse_type.values[j]}]  \" + t[start:end] + f\" [END_{g.discourse_type.values[j]}] \" + t[end:] \n",
    "                    else:\n",
    "                        t = t[:start] + f\" [START] {g.discourse_type.values[j]} \" + t[start:end] + \" [END] \" + t[end:] \n",
    "\n",
    "                if hasattr(self.cfg.dataset, \"add_group_types\") and self.cfg.dataset.add_group_types:\n",
    "                    t = \" \".join(g.discourse_type.values) + f\" {self.cfg._tokenizer_sep_token} \" + t\n",
    "                        \n",
    "                self.grp_texts.append(t)\n",
    "\n",
    "                s += len(g)\n",
    "\n",
    "            if self.cfg.dataset.group_discourse:\n",
    "                \n",
    "                self.cfg._tokenizer_start_token_id = []\n",
    "                self.cfg._tokenizer_end_token_id = []\n",
    "\n",
    "                if self.cfg.architecture.use_type:\n",
    "                    for type in sorted(self.df.discourse_type.unique()):\n",
    "                        self.tokenizer.add_tokens([f\"[START_{type}]\"], special_tokens=True)\n",
    "                        self.cfg._tokenizer_start_token_id.append(self.tokenizer.encode(f\"[START_{type}]\")[1])\n",
    "                    \n",
    "                    for type in sorted(self.df.discourse_type.unique()):\n",
    "                        self.tokenizer.add_tokens([f\"[END_{type}]\"], special_tokens=True)\n",
    "                        self.cfg._tokenizer_end_token_id.append(self.tokenizer.encode(f\"[END_{type}]\")[1])\n",
    "\n",
    "                else:\n",
    "                    self.tokenizer.add_tokens([\"[START]\", \"[END]\"], special_tokens=True)\n",
    "                    self.cfg._tokenizer_start_token_id.append(self.tokenizer.encode(f\"[START]\")[1])\n",
    "                    self.cfg._tokenizer_end_token_id.append(self.tokenizer.encode(f\"[END]]\")[1])\n",
    "\n",
    "                print(self.cfg._tokenizer_start_token_id)\n",
    "                print(self.cfg._tokenizer_end_token_id)\n",
    "\n",
    "            if hasattr(self.cfg.tokenizer, \"add_newline_token\") and self.cfg.tokenizer.add_newline_token:\n",
    "                self.tokenizer.add_tokens([f\"\\n\"], special_tokens=True)\n",
    "\n",
    "            self.cfg._tokenizer_size = len(self.tokenizer)\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            return len(self.grp_texts)\n",
    "        else:\n",
    "            return len(self.df)\n",
    "        \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        elem = batch[0]\n",
    "\n",
    "        ret = {}\n",
    "        for key in elem:\n",
    "            if key in {\"target\", \"weight\"}:\n",
    "                ret[key] = [d[key].float() for d in batch]\n",
    "            elif key in {\"target_aux\"}:\n",
    "\n",
    "                ret[key] = [d[key].float() for d in batch]\n",
    "            else:\n",
    "                ret[key] = torch.stack([d[key] for d in batch], 0)\n",
    "        return ret\n",
    "            \n",
    "    def batch_to_device(batch, device):\n",
    "\n",
    "        if isinstance(batch, torch.Tensor):\n",
    "            return batch.to(device)\n",
    "        elif isinstance(batch, collections.abc.Mapping):\n",
    "            return {\n",
    "                key: FeedbackDataset.batch_to_device(value, device)\n",
    "                for key, value in batch.items()\n",
    "            }\n",
    "        elif isinstance(batch, collections.abc.Sequence):\n",
    "            return [FeedbackDataset.batch_to_device(value, device) for value in batch]\n",
    "        else:\n",
    "            raise ValueError(f\"Can not move {type(batch)} to device.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _lowercase(sample):\n",
    "        if isinstance(sample, str):\n",
    "            return sample.lower()\n",
    "        elif isinstance(sample, Iterable):\n",
    "            return [x.lower() for x in sample]\n",
    "    \n",
    "    def get_texts(cls, df, cfg, separator):\n",
    "        if separator is None:\n",
    "            if hasattr(cfg.dataset, \"separator\") and len(cfg.dataset.separator):\n",
    "                separator = cfg.dataset.separator\n",
    "            else:\n",
    "                separator = getattr(cfg, \"_tokenizer_sep_token\", \"<SEPARATOR>\")\n",
    "\n",
    "        lowercase = hasattr(cfg, \"tokenizer\") and cfg.tokenizer.lowercase\n",
    "        if isinstance(cfg.dataset.text_column, str):\n",
    "            texts = df[cfg.dataset.text_column].astype(str)\n",
    "            if lowercase:\n",
    "                texts = texts.apply(cls._lowercase)\n",
    "            texts = texts.values\n",
    "        else:\n",
    "            columns = list(cfg.dataset.text_column)\n",
    "            join_str = f\" {separator} \"\n",
    "            texts = df[columns].astype(str)\n",
    "            if lowercase:\n",
    "                texts = texts.apply(cls._lowercase)\n",
    "            texts = texts.apply(lambda x: join_str.join(x), axis=1).values\n",
    "\n",
    "        return texts\n",
    "        \n",
    "    def _read_data(self, idx, sample):\n",
    "\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            text = self.grp_texts[idx]\n",
    "        else:\n",
    "            text = self.text[idx]\n",
    "\n",
    "        if idx == 0:\n",
    "            print(text)\n",
    "            \n",
    "        sample.update(self.encode(text))\n",
    "        return sample\n",
    "    \n",
    "    def encode(self, text):\n",
    "        sample = dict()\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.cfg.tokenizer.max_length,\n",
    "        )\n",
    "        sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n",
    "        sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict()\n",
    "            \n",
    "        sample = self._read_data(idx=idx, sample=sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d038c",
   "metadata": {
    "papermill": {
     "duration": 0.04944,
     "end_time": "2022-08-22T20:15:48.459314",
     "exception": false,
     "start_time": "2022-08-22T20:15:48.409874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "class NLPAllclsTokenPooling(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, cfg):\n",
    "        super(NLPAllclsTokenPooling, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.feat_mult = 1\n",
    "        if cfg.dataset.group_discourse:\n",
    "            self.feat_mult = 3\n",
    "\n",
    "    def forward(self, x, attention_mask, input_ids, cfg):\n",
    "\n",
    "        if not cfg.dataset.group_discourse:\n",
    "            input_ids_expanded = input_ids.clone().unsqueeze(-1).expand(x.shape)\n",
    "            attention_mask_expanded = torch.zeros_like(input_ids_expanded)\n",
    "\n",
    "            attention_mask_expanded[(input_ids_expanded == cfg._tokenizer_cls_token_id) | (input_ids_expanded == cfg._tokenizer_sep_token_id)] = 1\n",
    "\n",
    "            sum_features = (x * attention_mask_expanded).sum(self.dim)\n",
    "            ret = sum_features / attention_mask_expanded.sum(self.dim).clip(min=1e-8)\n",
    "\n",
    "        else:\n",
    "            ret = []\n",
    "\n",
    "            for j in range(x.shape[0]):\n",
    "\n",
    "\n",
    "                idx0 = torch.where((input_ids[j] >= min(cfg._tokenizer_start_token_id)) & (input_ids[j] <= max(cfg._tokenizer_start_token_id)))[0]\n",
    "                idx1 = torch.where((input_ids[j] >= min(cfg._tokenizer_end_token_id)) & (input_ids[j] <= max(cfg._tokenizer_end_token_id)))[0]\n",
    "\n",
    "                xx = []\n",
    "                for jj in range(len(idx0)):\n",
    "                    xx0 = x[j, idx0[jj]]\n",
    "                    xx1 = x[j, idx1[jj]]\n",
    "                    xx2 = x[j, idx0[jj]+1:idx1[jj]].mean(dim=0)\n",
    "                    xxx = torch.cat([xx0, xx1, xx2]).unsqueeze(0)\n",
    "                    xx.append(xxx)\n",
    "                xx = torch.cat(xx)\n",
    "                ret.append(xx)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim, cfg, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, x, attention_mask, input_ids, cfg):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n",
    "        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret\n",
    "    \n",
    "class NLPPoolings:\n",
    "    _poolings = {\n",
    "        \"All [CLS] token\": NLPAllclsTokenPooling,\n",
    "        \"GeM\": GeMText\n",
    "    }\n",
    "    @classmethod\n",
    "    def get(cls, name):\n",
    "        return cls._poolings.get(name)\n",
    "\n",
    "class FeedbackModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        super(FeedbackModel, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.n_classes = 3\n",
    "        config = AutoConfig.from_pretrained(cfg.architecture.cache_dir)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "    \n",
    "        self.backbone.pooler = None\n",
    "        \n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            self.backbone.resize_token_embeddings(cfg._tokenizer_size)\n",
    "        \n",
    "        self.pooling = NLPPoolings.get(self.cfg.architecture.pool)\n",
    "        self.pooling = self.pooling(dim=1, cfg=cfg)  # init pooling and pool over token dimension\n",
    "        \n",
    "        self.head = nn.Linear(self.backbone.config.hidden_size*self.pooling.feat_mult, self.n_classes)\n",
    "\n",
    "    def get_features(self, batch):\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "\n",
    "        x = self.backbone(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        x = self.pooling(x, attention_mask, input_ids, cfg=self.cfg)\n",
    "\n",
    "        if self.cfg.dataset.group_discourse:\n",
    "            x = torch.cat(x)\n",
    "        \n",
    "        if self.cfg.architecture.dropout > 0.0:\n",
    "            x = F.dropout(x, p=self.cfg.architecture.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, batch, calculate_loss=False):\n",
    "        \n",
    "        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max())\n",
    "        idx += 1\n",
    "        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n",
    "        \n",
    "        x = self.get_features(batch)\n",
    "                \n",
    "        logits = self.head(x)\n",
    "        outputs = {}\n",
    "\n",
    "        outputs[\"logits\"] = logits\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302b9db",
   "metadata": {
    "papermill": {
     "duration": 0.988113,
     "end_time": "2022-08-22T20:15:49.468740",
     "exception": false,
     "start_time": "2022-08-22T20:15:48.480627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, PreTrainedModel\n",
    "\n",
    "def create_nlp_backbone(\n",
    "    cfg, model_class=AutoModel, remove_pooling_layer=False\n",
    "):\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        cfg[\"backbone\"], cache_dir=cfg[\"cache_dir\"]\n",
    "    )\n",
    "\n",
    "    kwargs = dict(add_pooling_layer=False) if remove_pooling_layer else dict()\n",
    "    \n",
    "    try:\n",
    "        backbone = model_class.from_config(config, **kwargs)\n",
    "    except TypeError:\n",
    "        backbone = model_class.from_config(config)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "def glorot_uniform(parameter):\n",
    "    nn.init.xavier_uniform_(parameter.data, gain=1.0)\n",
    "\n",
    "\n",
    "class NBMEHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NBMEHead, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(input_dim, output_dim)\n",
    "        glorot_uniform(self.classifier.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is B x S x C\n",
    "        logits1 = self.classifier(self.dropout1(x))\n",
    "        logits2 = self.classifier(self.dropout2(x))\n",
    "        logits3 = self.classifier(self.dropout3(x))\n",
    "        logits4 = self.classifier(self.dropout4(x))\n",
    "        logits5 = self.classifier(self.dropout5(x))\n",
    "\n",
    "        logits = ((logits1 + logits2 + logits3 + logits4 + logits5) / 5)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ModelYauhen(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ModelYauhen, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.n_classes = 3\n",
    "        self.backbone = create_nlp_backbone(\n",
    "            self.cfg,\n",
    "            model_class=AutoModel,\n",
    "            remove_pooling_layer=False,\n",
    "        )\n",
    "        self.head = nn.Linear(self.backbone.config.hidden_size, 3)\n",
    "        if self.cfg[\"add_wide_dropout\"]:\n",
    "            self.token_type_head = NBMEHead(self.backbone.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, batch, calculate_loss=True):\n",
    "        outputs = {}\n",
    "        \n",
    "        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max()) \n",
    "        idx += 1\n",
    "        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n",
    "        batch[\"word_start_mask\"] = batch[\"word_start_mask\"][:, :idx]\n",
    "        batch[\"word_ids\"] = batch[\"word_ids\"][:, :idx]\n",
    "\n",
    "        outputs[\"word_start_mask\"] = batch[\"word_start_mask\"]\n",
    "\n",
    "        x = self.backbone(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        ).last_hidden_state\n",
    "\n",
    "        for obs_id in range(x.size()[0]):\n",
    "            for w_id in range(int(torch.max(batch[\"word_ids\"][obs_id]).item()) + 1):\n",
    "                chunk_mask = batch[\"word_ids\"][obs_id] == w_id\n",
    "                chunk_logits = x[obs_id] * chunk_mask.unsqueeze(-1)\n",
    "                chunk_logits = chunk_logits.sum(dim=0) / chunk_mask.sum()\n",
    "                x[obs_id][chunk_mask] = chunk_logits\n",
    "\n",
    "        if self.cfg[\"add_wide_dropout\"]:\n",
    "            logits = self.token_type_head(x)\n",
    "        else:\n",
    "            logits = self.head(x)\n",
    "        outputs[\"logits\"] = logits      \n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class FeedbackDatasetYauhen(Dataset):\n",
    "    @staticmethod\n",
    "    def _lowercase(sample):\n",
    "        if isinstance(sample, str):\n",
    "            return sample.lower()\n",
    "        elif isinstance(sample, Iterable):\n",
    "            return [x.lower() for x in sample]\n",
    "\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.cfg[\"backbone\"],\n",
    "            add_prefix_space=True,\n",
    "            use_fast=True,\n",
    "            cache_dir=self.cfg[\"cache_dir\"],\n",
    "        )\n",
    "\n",
    "        self.text = self.get_texts(self.df, self.cfg, self.tokenizer.sep_token)\n",
    "        self.labels = self.df[\"tokens\"].values\n",
    "\n",
    "    @classmethod\n",
    "    def get_texts(cls, df, cfg, separator=None):\n",
    "        texts = df[cfg[\"text_column\"]].values\n",
    "\n",
    "        if cfg[\"lowercase\"]:\n",
    "            texts = [cls._lowercase(x) for x in texts]\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict()\n",
    "            \n",
    "        text = self.text[idx]\n",
    "        \n",
    "        if \"deberta-v3\" in self.cfg[\"backbone\"]:\n",
    "            text = [x.replace(\"\\n\", \"[NL_HYDRO]\") for x in list(text)]\n",
    "            text = [x if not x.isspace() else \"[SP_HYDRO]\" * len(x) for x in text]\n",
    "            tokenizer_input = [text]\n",
    "            raise ValueError(f\"BES {text}\")\n",
    "        else:\n",
    "            if \"add_types\" in self.cfg and self.cfg[\"add_types\"]:\n",
    "                tokenizer_input = [x if x_idx > 0 else x + self.tokenizer.sep_token for x_idx, x in enumerate(list(text))]\n",
    "            else:\n",
    "                tokenizer_input = [list(text)]\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            tokenizer_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "\n",
    "        sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n",
    "        sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n",
    "\n",
    "        word_ids = encodings.word_ids(0)\n",
    "        word_ids = [-1 if x is None else x for x in word_ids]\n",
    "        sample[\"word_ids\"] = torch.tensor(word_ids)\n",
    "\n",
    "        word_start_mask = []\n",
    "        lab_idx = -1\n",
    "        for i, word in enumerate(word_ids):\n",
    "            word_start = word > -1 and (i == 0 or word_ids[i - 1] != word)\n",
    "            if word_start:\n",
    "                lab_idx += 1\n",
    "                if self.labels[idx][lab_idx] != 1:\n",
    "                    word_start_mask.append(True)\n",
    "                    continue\n",
    "\n",
    "            word_start_mask.append(False)\n",
    "\n",
    "        sample[\"word_start_mask\"] = torch.tensor(word_start_mask)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929125b3",
   "metadata": {
    "papermill": {
     "duration": 0.028343,
     "end_time": "2022-08-22T20:15:49.517922",
     "exception": false,
     "start_time": "2022-08-22T20:15:49.489579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad2241",
   "metadata": {
    "papermill": {
     "duration": 0.080798,
     "end_time": "2022-08-22T20:15:49.618171",
     "exception": false,
     "start_time": "2022-08-22T20:15:49.537373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_obs = []\n",
    "\n",
    "for name, gr in tqdm(test.groupby(\"essay_id\", sort=False)):\n",
    "    essay_text_start_end = gr.essay_text.values[0]\n",
    "    token_labels = []\n",
    "    token_obs = []\n",
    "    end_pos = 0\n",
    "    \n",
    "    for idx, row in gr.reset_index(drop=True).iterrows():\n",
    "        target_text = row[\"discourse_type\"] + \" \" + row[\"discourse_text\"].strip()\n",
    "        \n",
    "        essay_text_start_end = essay_text_start_end[:end_pos] + essay_text_start_end[end_pos:].replace(row[\"discourse_text\"].strip(), target_text, 1)\n",
    "        \n",
    "        start_pos = essay_text_start_end[end_pos:].find(target_text)\n",
    "        if start_pos == -1:\n",
    "            raise ValueError()\n",
    "        start_pos += end_pos\n",
    "        \n",
    "        if idx == 0 and start_pos > 0:\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[:start_pos])\n",
    "        \n",
    "        if start_pos > end_pos and end_pos > 0:\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[end_pos:start_pos])\n",
    "  \n",
    "        end_pos = start_pos + len(target_text)\n",
    "        token_labels.append(0)\n",
    "        token_obs.append(essay_text_start_end[start_pos: end_pos])\n",
    "            \n",
    "        if idx == len(gr) - 1 and end_pos < len(essay_text_start_end):\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[end_pos:])\n",
    "            \n",
    "    if len(token_labels) != len(token_obs):\n",
    "        raise ValueError()\n",
    "            \n",
    "    all_obs.append((name, token_labels, token_obs))\n",
    "\n",
    "tt = pd.DataFrame(all_obs, columns=[\"essay_id\", \"tokens\", \"essay_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3c12b",
   "metadata": {
    "papermill": {
     "duration": 0.086172,
     "end_time": "2022-08-22T20:15:49.724622",
     "exception": false,
     "start_time": "2022-08-22T20:15:49.638450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_obs = []\n",
    "\n",
    "for name, gr in tqdm(test.groupby(\"essay_id\", sort=False)):\n",
    "    essay_text_start_end = gr.essay_text.values[0]\n",
    "    token_labels = []\n",
    "    token_obs = []\n",
    "    end_pos = 0\n",
    "    \n",
    "    token_obs.append(\" \".join(gr.discourse_type.to_list()))\n",
    "    token_labels.append(1)\n",
    "    \n",
    "    for idx, row in gr.reset_index(drop=True).iterrows():\n",
    "        target_text = row[\"discourse_type\"] + \" \" + row[\"discourse_text\"].strip()\n",
    "        \n",
    "        essay_text_start_end = essay_text_start_end[:end_pos] + essay_text_start_end[end_pos:].replace(row[\"discourse_text\"].strip(), target_text, 1)\n",
    "        \n",
    "        start_pos = essay_text_start_end[end_pos:].find(target_text)\n",
    "        if start_pos == -1:\n",
    "            raise ValueError()\n",
    "        start_pos += end_pos\n",
    "        \n",
    "        if idx == 0 and start_pos > 0:\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[:start_pos])\n",
    "        \n",
    "        if start_pos > end_pos and end_pos > 0:\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[end_pos:start_pos])\n",
    "  \n",
    "        end_pos = start_pos + len(target_text)\n",
    "        token_labels.append(0)\n",
    "        token_obs.append(essay_text_start_end[start_pos: end_pos])\n",
    "            \n",
    "        if idx == len(gr) - 1 and end_pos < len(essay_text_start_end):\n",
    "            token_labels.append(1)\n",
    "            token_obs.append(essay_text_start_end[end_pos:])\n",
    "            \n",
    "    if len(token_labels) != len(token_obs):\n",
    "        raise ValueError()\n",
    "            \n",
    "    all_obs.append((name, token_labels, token_obs))\n",
    "\n",
    "tt_v2 = pd.DataFrame(all_obs, columns=[\"essay_id\", \"tokens\", \"essay_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadba5d3",
   "metadata": {
    "papermill": {
     "duration": 0.042091,
     "end_time": "2022-08-22T20:15:49.789536",
     "exception": false,
     "start_time": "2022-08-22T20:15:49.747445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_predictions_philipp(exp_name, df, BS=1):\n",
    "    \n",
    "    cfg = yaml.safe_load(open(f\"../input/{exp_name}/cfg-fold0.yaml\").read())\n",
    "    for k,v in cfg.items():\n",
    "        if type(v) == dict:\n",
    "            cfg[k] = SimpleNamespace(**v)\n",
    "    cfg = SimpleNamespace(**cfg)\n",
    "\n",
    "    if cfg.architecture.backbone == 'microsoft/deberta-v3-large':\n",
    "        cfg.architecture.cache_dir = \"../input/deberta-v3-large/\"\n",
    "    elif cfg.architecture.backbone == 'microsoft/deberta-v3-small':\n",
    "        cfg.architecture.cache_dir = \"../input/deberta-v3-small/\"\n",
    "    elif cfg.architecture.backbone == 'microsoft/deberta-v3-base':\n",
    "        cfg.architecture.cache_dir = \"../input/deberta-v3-lbase/\"\n",
    "\n",
    "    ds = FeedbackDataset(df.iloc[:], mode=\"test\", cfg=cfg)\n",
    "    \n",
    "    preds_all = []\n",
    "    for fold in range(NUM_MODELS):\n",
    "        print(f\"running model {fold}\")\n",
    "        \n",
    "        model = FeedbackModel(cfg).to(\"cuda\").eval()\n",
    "    \n",
    "        d = torch.load(f\"../input/{exp_name}/checkpoint-fold{fold}.pth\", map_location=\"cpu\")\n",
    "\n",
    "        model_weights = d[\"model\"]\n",
    "        model_weights = {k.replace(\"module.\", \"\"): v for k, v in model_weights.items()}\n",
    "        \n",
    "        for k in list(model_weights.keys()):\n",
    "            if \"aux\" in k or \"loss_fn\" in k:\n",
    "                del model_weights[k]\n",
    "\n",
    "        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n",
    "        \n",
    "        del d\n",
    "        del model_weights\n",
    "        gc.collect() \n",
    "    \n",
    "        batch_size = BS\n",
    "        dl = DataLoader(ds, shuffle=False, batch_size = batch_size, num_workers = N_CORES)\n",
    "\n",
    "        with torch.no_grad():    \n",
    "            preds = []\n",
    "            for batch in tqdm(dl):\n",
    "\n",
    "                batch = FeedbackDataset.batch_to_device(batch, \"cuda\")\n",
    "                out = model(batch)\n",
    "                preds.append(out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        preds_all.append(np.concatenate(preds, axis=0))\n",
    "        \n",
    "        del model\n",
    "        del dl\n",
    "        gc.collect()\n",
    "        \n",
    "    del ds\n",
    "    \n",
    "    \n",
    "    preds = np.mean(preds_all, axis=0)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def run_predictions_yauhen(all_cfgs, df, yauhen_batch_size=1):\n",
    "    ds = FeedbackDatasetYauhen(df=df, cfg=all_cfgs[0])\n",
    "        \n",
    "    preds_all = []\n",
    "\n",
    "    for params in all_cfgs[:NUM_MODELS]:\n",
    "\n",
    "        model = ModelYauhen(params).to(\"cuda\").eval()\n",
    "\n",
    "        d = torch.load(params[\"path\"], map_location=\"cpu\")\n",
    "\n",
    "        model_weights = d[\"model\"]\n",
    "        model_weights = {k.replace(\"module.\", \"\"): v for k, v in model_weights.items()}\n",
    "        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n",
    "        \n",
    "        del d\n",
    "        del model_weights\n",
    "        gc.collect() \n",
    "    \n",
    "        dl = DataLoader(ds, shuffle=False, batch_size = yauhen_batch_size, num_workers = N_CORES)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds = []\n",
    "            for batch in tqdm(dl):\n",
    "                texts = {\n",
    "                key: value.to(\"cuda\")\n",
    "                for key, value in batch.items()\n",
    "            }\n",
    "                output = model.forward(texts, calculate_loss=False)\n",
    "\n",
    "                val = (\n",
    "                        torch.softmax(output[\"logits\"][output[\"word_start_mask\"]], dim=1).detach().cpu().numpy()\n",
    "                    )\n",
    "\n",
    "                preds.append(val)\n",
    "\n",
    "        preds_all.append(np.concatenate(preds, axis=0))\n",
    "        \n",
    "        del model\n",
    "        del dl\n",
    "        gc.collect()\n",
    "        \n",
    "    del ds\n",
    "    \n",
    "    preds = np.mean(preds_all, axis=0)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24158167",
   "metadata": {
    "papermill": {
     "duration": 0.028089,
     "end_time": "2022-08-22T20:15:49.837710",
     "exception": false,
     "start_time": "2022-08-22T20:15:49.809621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f74da9",
   "metadata": {
    "papermill": {
     "duration": 0.029695,
     "end_time": "2022-08-22T20:15:49.888385",
     "exception": false,
     "start_time": "2022-08-22T20:15:49.858690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_probs(pp_single, scale):\n",
    "    pp = pp_single.copy()\n",
    "\n",
    "    for jj,s in enumerate(scale):\n",
    "        pp[:,jj] = pp[:,jj] * s\n",
    "\n",
    "    pp = pp / pp.sum(axis=1, keepdims=True)\n",
    "        \n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbf664",
   "metadata": {
    "papermill": {
     "duration": 4.519715,
     "end_time": "2022-08-22T20:15:54.428918",
     "exception": false,
     "start_time": "2022-08-22T20:15:49.909203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "def get_features(df, vects):\n",
    "    vect_discourse = vects[\"vect_discourse\"]\n",
    "    X = vect_discourse.transform(df[\"discourse_text\"]).A\n",
    "\n",
    "    X = np.concatenate([X, X.sum(axis=1).reshape(-1,1)], axis=1)\n",
    "\n",
    "    vect_essay = vects[\"vect_essay\"]\n",
    "    XX = vect_essay.transform(df[\"essay_text\"]).A\n",
    "    XX = np.concatenate([XX, XX.sum(axis=1).reshape(-1,1)], axis=1)\n",
    "    X = np.concatenate([X, XX], axis=1)\n",
    "\n",
    "    vect_type = vects[\"vect_type\"]\n",
    "    X = np.concatenate([X, vect_type.transform(df[\"discourse_type\"]).A], axis=1)\n",
    "\n",
    "    vect_type_essay = vects[\"vect_type_essay\"]\n",
    "    X = np.concatenate([X, vect_type_essay.transform(df[\"discourse_type_essay\"]).A], axis=1)\n",
    "\n",
    "    f = \"rel_rank\"\n",
    "    X = np.concatenate([X, df[f].values.reshape(-1,1)], axis=1)\n",
    "    print(X.shape)\n",
    "\n",
    "    return X\n",
    "\n",
    "def run_predictions_lgb(exp_name, df):\n",
    "    df[\"rank\"] = df.groupby(\"essay_id\")[\"discourse_type\"].transform(lambda x: np.arange(len(x))).values\n",
    "    df[\"length\"] = df.groupby(\"essay_id\")[\"discourse_type\"].transform(lambda x: len(x)).values\n",
    "    df[\"rel_rank\"] = df[\"rank\"] / df[\"length\"]\n",
    "    \n",
    "    pps = []\n",
    "    for fold in [-1]:\n",
    "    \n",
    "        vects = pd.read_pickle(f\"../input/{exp_name}/fold{fold}/vectorizers.p\")\n",
    "\n",
    "        X = get_features(df, vects)\n",
    "\n",
    "        clf = lightgbm.Booster(model_file=f\"../input/{exp_name}/fold{fold}/model_seed0.txt\")\n",
    "\n",
    "        preds = clf.predict(X)\n",
    "        pps.append(preds)\n",
    "    preds = np.mean(pps, axis=0)\n",
    "    \n",
    "    print(preds.mean(axis=0))\n",
    "    \n",
    "    return preds\n",
    "\n",
    "preds.append(scale_probs(run_predictions_lgb(\"lgb-v0\", df), [0.88599866, 1.16932577, 0.97401762]))\n",
    "weights.append(1.474412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b993f80",
   "metadata": {
    "papermill": {
     "duration": 92.180756,
     "end_time": "2022-08-22T20:17:26.631785",
     "exception": false,
     "start_time": "2022-08-22T20:15:54.451029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold0.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold1.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold2.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "preds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt, yauhen_batch_size=4), [0.86152501, 0.84374834, 1.19471905]))\n",
    "weights.append(-2.507774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3a792",
   "metadata": {
    "papermill": {
     "duration": 176.867057,
     "end_time": "2022-08-22T20:20:23.523165",
     "exception": false,
     "start_time": "2022-08-22T20:17:26.656108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/saffron-rook-ff/checkpoint_fold_0.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/saffron-rook-ff/checkpoint_fold_1.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/saffron-rook-ff/checkpoint_fold_2.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "seed_1 = run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4)\n",
    "\n",
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/saffron-rook-v2-ff/checkpoint_fold_0.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/saffron-rook-v2-ff/checkpoint_fold_1.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/saffron-rook-v2-ff/checkpoint_fold_2.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "seed_2 = run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4)\n",
    "\n",
    "seeds = (seed_1 + seed_2) / 2\n",
    "\n",
    "preds.append(scale_probs(seeds, [0.93795684, 0.84181179, 1.17626508]))\n",
    "weights.append(0.928444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0edde",
   "metadata": {
    "papermill": {
     "duration": 159.459869,
     "end_time": "2022-08-22T20:23:03.013583",
     "exception": false,
     "start_time": "2022-08-22T20:20:23.553714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertaxlarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertaxlarge\",\n",
    "       \"path\": \"../input/big-ocelot-ff/checkpoint_fold_0.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertaxlarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertaxlarge\",\n",
    "       \"path\": \"../input/big-ocelot-ff/checkpoint_fold_1.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertaxlarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertaxlarge\",\n",
    "       \"path\": \"../input/big-ocelot-ff/checkpoint_fold_2.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "preds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4), [0.97697565, 0.80606502, 1.28081784]))\n",
    "weights.append(2.151761)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e2517",
   "metadata": {
    "papermill": {
     "duration": 88.034444,
     "end_time": "2022-08-22T20:24:31.079764",
     "exception": false,
     "start_time": "2022-08-22T20:23:03.045320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/shrewd-rook-3ep-ff/checkpoint-fold0.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/shrewd-rook-3ep-ff/checkpoint-fold1.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/shrewd-rook-3ep-ff/checkpoint-fold2.pth\",\n",
    "         \"add_wide_dropout\": False,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "preds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt, yauhen_batch_size=4), [0.85699376, 0.72677681, 1.28469071]))\n",
    "weights.append(0.992435)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a481607",
   "metadata": {
    "papermill": {
     "duration": 87.426312,
     "end_time": "2022-08-22T20:25:58.539032",
     "exception": false,
     "start_time": "2022-08-22T20:24:31.112720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/conscious-uakari-ff/checkpoint-fold0.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/conscious-uakari-ff/checkpoint-fold1.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/conscious-uakari-ff/checkpoint-fold2.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "\n",
    "preds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4), [0.95308328, 0.76116742, 1.24672952]))\n",
    "weights.append(6.144367)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339ccf9",
   "metadata": {
    "papermill": {
     "duration": 157.544229,
     "end_time": "2022-08-22T20:28:36.118972",
     "exception": false,
     "start_time": "2022-08-22T20:25:58.574743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertaxlarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertaxlarge\",\n",
    "       \"path\": \"../input/wondrous-stoat-ff/checkpoint-fold0.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertaxlarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertaxlarge\",\n",
    "       \"path\": \"../input/wondrous-stoat-ff/checkpoint-fold1.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertaxlarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertaxlarge\",\n",
    "       \"path\": \"../input/wondrous-stoat-ff/checkpoint-fold2.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "preds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4), [0.96222241, 0.79957189, 1.3173084 ]))\n",
    "weights.append(1.171310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83663c",
   "metadata": {
    "papermill": {
     "duration": 83.687303,
     "end_time": "2022-08-22T20:29:59.844002",
     "exception": false,
     "start_time": "2022-08-22T20:28:36.156699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_1 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/thoughtful-sturgeon-ff-v2/checkpoint-fold0.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_2 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/thoughtful-sturgeon-ff-v2/checkpoint-fold1.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "cfg_3 = {\"backbone\": \"../input/debertalarge\",\n",
    "       \"lowercase\": False,\n",
    "       \"text_column\": \"essay_text\",\n",
    "       \"cache_dir\": \"../input/debertalarge\",\n",
    "       \"path\": \"../input/thoughtful-sturgeon-ff-v2/checkpoint-fold2.pth\",\n",
    "         \"add_wide_dropout\": True,\n",
    "         \"add_types\": True,\n",
    "      }\n",
    "\n",
    "all_cfgs = [cfg_1, cfg_2, cfg_3]\n",
    "\n",
    "preds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4), [0.93894504, 0.83679594, 1.17373473]))\n",
    "weights.append(-6.435434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb8c5c",
   "metadata": {
    "papermill": {
     "duration": 106.812331,
     "end_time": "2022-08-22T20:31:46.696779",
     "exception": false,
     "start_time": "2022-08-22T20:29:59.884448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"valiant-degu-ff-2\", df, BS=8), [1.00590236, 0.74325877, 1.23595038]))\n",
    "weights.append(0.784960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4bab9",
   "metadata": {
    "papermill": {
     "duration": 106.620473,
     "end_time": "2022-08-22T20:33:33.365566",
     "exception": false,
     "start_time": "2022-08-22T20:31:46.745093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"axiomatic-vulture-ff\", df, BS=8), [0.8990305 , 0.81126708, 1.20530452]))\n",
    "weights.append(1.014111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4686fe3",
   "metadata": {
    "papermill": {
     "duration": 104.947885,
     "end_time": "2022-08-22T20:35:18.363873",
     "exception": false,
     "start_time": "2022-08-22T20:33:33.415988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"awesome-rose-ff\", df, BS=8), [0.87750372, 0.52199115, 1.39935137]))\n",
    "weights.append(1.520703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d469e08",
   "metadata": {
    "papermill": {
     "duration": 105.56866,
     "end_time": "2022-08-22T20:37:03.984529",
     "exception": false,
     "start_time": "2022-08-22T20:35:18.415869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"honest-apple-ff\", df, BS=8), [1.05735578, 0.99475725, 0.9675566 ]))\n",
    "weights.append(0.932937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a62ed",
   "metadata": {
    "papermill": {
     "duration": 105.935456,
     "end_time": "2022-08-22T20:38:49.975182",
     "exception": false,
     "start_time": "2022-08-22T20:37:04.039726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"funky-funk-ff\", df, BS=8), [0.91628124, 0.47570106, 1.37440505]))\n",
    "weights.append(1.201942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c649b",
   "metadata": {
    "papermill": {
     "duration": 103.816018,
     "end_time": "2022-08-22T20:40:33.845413",
     "exception": false,
     "start_time": "2022-08-22T20:38:50.029395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"lame-flame-ff\", df, BS=8), [0.95149563, 0.78844189, 1.21126386]))\n",
    "weights.append(2.392048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07447deb",
   "metadata": {
    "papermill": {
     "duration": 43.642652,
     "end_time": "2022-08-22T20:41:17.550160",
     "exception": false,
     "start_time": "2022-08-22T20:40:33.907508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"pastel-frog-ff\", df, BS=8), [0.92647706, 0.67330446, 1.31772773]))\n",
    "weights.append(-1.782858)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39215e",
   "metadata": {
    "papermill": {
     "duration": 44.043901,
     "end_time": "2022-08-22T20:42:01.653438",
     "exception": false,
     "start_time": "2022-08-22T20:41:17.609537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"pastel-trail-ff\", df, BS=8), [1.01242026, 0.59306331, 1.32216907]))\n",
    "weights.append(2.282727)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe08245",
   "metadata": {
    "papermill": {
     "duration": 99.582952,
     "end_time": "2022-08-22T20:43:41.295224",
     "exception": false,
     "start_time": "2022-08-22T20:42:01.712272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.append(scale_probs(run_predictions_philipp(\"prophetic-hedgehog-large-ff\", df, BS=8), [0.97797295, 0.97287244, 1.06829884]))\n",
    "weights.append(3.701103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673365a8",
   "metadata": {
    "papermill": {
     "duration": 0.071326,
     "end_time": "2022-08-22T20:43:41.430703",
     "exception": false,
     "start_time": "2022-08-22T20:43:41.359377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_orig = np.array(preds).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22be95",
   "metadata": {
    "papermill": {
     "duration": 0.086751,
     "end_time": "2022-08-22T20:43:41.579302",
     "exception": false,
     "start_time": "2022-08-22T20:43:41.492551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ind_models = df.copy()\n",
    "\n",
    "for model_idx in range(len(preds)):\n",
    "    df_ind_models[f\"Adequate_{model_idx}\"] = preds[model_idx][:,0]\n",
    "    df_ind_models[f\"Effective_{model_idx}\"] = preds[model_idx][:,1]\n",
    "    df_ind_models[f\"Ineffective_{model_idx}\"] = preds[model_idx][:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c4b9c",
   "metadata": {
    "papermill": {
     "duration": 0.068426,
     "end_time": "2022-08-22T20:43:41.707787",
     "exception": false,
     "start_time": "2022-08-22T20:43:41.639361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = np.average(preds, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49da14",
   "metadata": {
    "papermill": {
     "duration": 0.068848,
     "end_time": "2022-08-22T20:43:42.408159",
     "exception": false,
     "start_time": "2022-08-22T20:43:42.339311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp = preds.copy()\n",
    "\n",
    "eps = 0.0001\n",
    "pp = pp.clip(eps, 1 - eps)\n",
    "pp = pp / pp.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238c36f",
   "metadata": {
    "papermill": {
     "duration": 0.069313,
     "end_time": "2022-08-22T20:43:42.805385",
     "exception": false,
     "start_time": "2022-08-22T20:43:42.736072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Adequate\"] = pp[:, 0] \n",
    "df[\"Effective\"] = pp[:, 1] \n",
    "df[\"Ineffective\"] = pp[:, 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba1c65",
   "metadata": {
    "papermill": {
     "duration": 0.069542,
     "end_time": "2022-08-22T20:43:42.933196",
     "exception": false,
     "start_time": "2022-08-22T20:43:42.863654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ind_models[\"Adequate\"] = pp[:, 0] \n",
    "df_ind_models[\"Effective\"] = pp[:, 1] \n",
    "df_ind_models[\"Ineffective\"] = pp[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac47f0",
   "metadata": {
    "papermill": {
     "duration": 0.116412,
     "end_time": "2022-08-22T20:43:43.166665",
     "exception": false,
     "start_time": "2022-08-22T20:43:43.050253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_preds = pp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bff662",
   "metadata": {
    "papermill": {
     "duration": 0.111472,
     "end_time": "2022-08-22T20:43:43.371883",
     "exception": false,
     "start_time": "2022-08-22T20:43:43.260411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CALC_SCORE:\n",
    "    from sklearn.metrics import log_loss\n",
    "    \n",
    "    label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n",
    "    \n",
    "    y = np.zeros_like(preds)\n",
    "    \n",
    "    for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n",
    "        y[ii,jj] = 1\n",
    "        \n",
    "    print(log_loss(y, pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a4581",
   "metadata": {
    "papermill": {
     "duration": 0.11552,
     "end_time": "2022-08-22T20:43:43.578927",
     "exception": false,
     "start_time": "2022-08-22T20:43:43.463407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n",
    "oof_cols = []\n",
    "for j, l in enumerate(label_cols):\n",
    "\n",
    "    df[f\"oof_{l}\"] = pp[:,j]\n",
    "    oof_cols.append(f\"oof_{l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156f438",
   "metadata": {
    "papermill": {
     "duration": 2.686225,
     "end_time": "2022-08-22T20:43:46.334329",
     "exception": false,
     "start_time": "2022-08-22T20:43:43.648104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedbackStackerModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(FeedbackStackerModel, self).__init__()\n",
    "        \n",
    "        self.sizes = [256, 128, 64]\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(n_features, self.sizes[0])),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(self.sizes[0], self.sizes[1]),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(self.sizes[1], self.sizes[2]),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Linear(self.sizes[-1], 3)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        \n",
    "        output = {}\n",
    "        \n",
    "        output[\"logits\"] = x\n",
    "        \n",
    "        if self.training:\n",
    "            output[\"loss\"] = self.loss_fn(x, y.argmax(dim=1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FeedbackStackerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, mode):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "\n",
    "        self.feature_cols = oof_cols.copy()\n",
    "        self.label_cols = label_cols.copy()\n",
    "        \n",
    "        df = self.df\n",
    "        \n",
    "        df[f\"len\"] = df.groupby(\"essay_id\")[f\"discourse_id\"].transform(\"count\") / 10\n",
    "        self.feature_cols.append(f\"len\")\n",
    "        \n",
    "        for j, l in enumerate(label_cols):\n",
    "            df[f\"oof_{l}_mean\"] = df.groupby(\"essay_id\")[f\"oof_{l}\"].transform(\"mean\")\n",
    "            self.feature_cols.append(f\"oof_{l}_mean\")\n",
    "            \n",
    "            df[f\"oof_{l}_t_mean\"] = df.groupby([\"essay_id\", \"discourse_type\"])[f\"oof_{l}\"].transform(\"mean\")\n",
    "            self.feature_cols.append(f\"oof_{l}_t_mean\")\n",
    "\n",
    "        self.num_features = len(self.feature_cols)\n",
    "\n",
    "        self.X = self.df[self.feature_cols].values\n",
    "        self.y = self.df[self.label_cols].values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "ds = FeedbackStackerDataset(df.copy(), mode=\"val\")\n",
    "ds[0][0].shape\n",
    "\n",
    "def run_nn_stacker(exp_name, df, BS=64):\n",
    "\n",
    "\n",
    "    ds = FeedbackStackerDataset(df.iloc[:].copy(), mode=\"test\")\n",
    "    \n",
    "    checkpoints = glob(f\"../input/{exp_name}/*.pth\")\n",
    "    \n",
    "    preds_all = []\n",
    "    for checkpoint in checkpoints:\n",
    "        print(f\"running model {checkpoint}\")\n",
    "        \n",
    "        model = FeedbackStackerModel(n_features=ds.num_features).to(\"cuda\").eval()\n",
    "    \n",
    "        model_weights = torch.load(checkpoint, map_location=\"cpu\")\n",
    "\n",
    "        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n",
    "        \n",
    "        del model_weights\n",
    "        gc.collect() \n",
    "    \n",
    "        batch_size = BS\n",
    "        dl = DataLoader(ds, shuffle=False, batch_size = batch_size, num_workers = N_CORES)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            for batch in tqdm(dl):\n",
    "\n",
    "                data = [x.to(\"cuda\") for x in batch]\n",
    "                inputs, target = data\n",
    "                out = model(inputs, target)\n",
    "                preds.append(out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        preds_all.append(np.concatenate(preds, axis=0))\n",
    "        \n",
    "        del model\n",
    "        del dl\n",
    "        gc.collect()\n",
    "        \n",
    "    del ds\n",
    "    \n",
    "    \n",
    "    preds = np.mean(preds_all, axis=0)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "nn_stacker_preds_1 = run_nn_stacker(\"feedback-nn-v8-blend162-ff\", df, BS=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143889cb",
   "metadata": {
    "papermill": {
     "duration": 8.872053,
     "end_time": "2022-08-22T20:43:55.270973",
     "exception": false,
     "start_time": "2022-08-22T20:43:46.398920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedbackStackerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, mode):\n",
    "        self.df = df.copy()\n",
    "        self.mode = mode\n",
    "\n",
    "        self.label_cols = label_cols.copy()\n",
    "        \n",
    "        p = [p[self.df.index.values] for p in preds_orig.copy()]\n",
    "        p = np.stack(p)\n",
    "        \n",
    "        df = self.df\n",
    "        \n",
    "        X = []\n",
    "        for j in range(p.shape[0]):\n",
    "            cols = []\n",
    "            for jj, l in enumerate(label_cols):\n",
    "\n",
    "                df[f\"oof_{l}\"] = p[j,:,jj]\n",
    "                cols.append(f\"oof_{l}\")\n",
    "                \n",
    "                df[f\"oof_{l}_mean\"] = df.groupby(\"essay_id\")[f\"oof_{l}\"].transform(\"mean\")\n",
    "                cols.append(f\"oof_{l}_mean\")\n",
    "\n",
    "                df[f\"oof_{l}_t_mean\"] = df.groupby([\"essay_id\", \"discourse_type\"])[f\"oof_{l}\"].transform(\"mean\")\n",
    "                cols.append(f\"oof_{l}_t_mean\")\n",
    "                \n",
    "            df[f\"len\"] = df.groupby(\"essay_id\")[f\"discourse_id\"].transform(\"count\") / 10\n",
    "            cols.append(f\"len\")\n",
    "        \n",
    "            \n",
    "            X.append(df[cols].values)\n",
    "         \n",
    "        X = np.stack(X).transpose(1,2,0)\n",
    "        print(X.shape)\n",
    "        \n",
    "        self.num_features = X.shape[1]\n",
    "\n",
    "        self.X = X\n",
    "        self.y = self.df[self.label_cols].values\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "class FeedbackStackerModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(FeedbackStackerModel, self).__init__()\n",
    "        \n",
    "        self.sizes = [256, 128, 64]\n",
    "        \n",
    "        layers = []\n",
    "        for j,s in enumerate(self.sizes):\n",
    "            if j == 0:\n",
    "                layers.append(nn.Conv1d(n_features, s, 1))\n",
    "            else:\n",
    "                layers.append(nn.Conv1d(self.sizes[j-1], s, 1))\n",
    "            layers.append(nn.PReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.head = nn.Linear(self.sizes[-1], 3)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        output = {}\n",
    "        \n",
    "        output[\"logits\"] = x\n",
    "        \n",
    "        if self.training:\n",
    "            output[\"loss\"] = self.loss_fn(x, y.argmax(dim=1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "ds = FeedbackStackerDataset(df.copy(), mode=\"val\")\n",
    "ds[0][0].shape\n",
    "\n",
    "nn_stacker_preds_2 = run_nn_stacker(\"feedback-nn-v11-blend162-ff\", df, BS=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea2184",
   "metadata": {
    "papermill": {
     "duration": 0.078979,
     "end_time": "2022-08-22T20:43:55.707125",
     "exception": false,
     "start_time": "2022-08-22T20:43:55.628146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_x(values):\n",
    "    range = 1\n",
    "    return np.histogram(np.clip(values, 0.001, 0.999*range), bins=3, density=True, range=(0,range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e156ca",
   "metadata": {
    "papermill": {
     "duration": 0.198636,
     "end_time": "2022-08-22T20:43:55.976012",
     "exception": false,
     "start_time": "2022-08-22T20:43:55.777376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_groups = []\n",
    "\n",
    "gb = df.groupby('essay_id', sort=False)\n",
    "for name, group in tqdm(gb):\n",
    "    group[\"n_types\"] = group.discourse_type.nunique()\n",
    "    for class_name in [\"Adequate\", \"Effective\", \"Ineffective\"]:\n",
    "        if class_name in [\"Adequate\", \"Effective\"]:\n",
    "            continue\n",
    "        for idx, val in enumerate(gen_x(group[class_name].values)):\n",
    "            group[f\"{class_name}_bin_{idx}\"] = val \n",
    "        group[f\"mean_{class_name}\"] = group[class_name].mean()    \n",
    "\n",
    "    all_groups.append(group)\n",
    "\n",
    "df = pd.concat(all_groups).reset_index(drop=True)\n",
    "\n",
    "disc_types_mapping = {'Lead': 0,\n",
    "'Position': 1,\n",
    "'Claim': 2,\n",
    "'Evidence': 3,\n",
    "'Counterclaim': 4,\n",
    "'Rebuttal': 5,\n",
    "'Concluding Statement': 6}\n",
    "df[\"len_disc\"] = df.discourse_text.str.len()\n",
    "\n",
    "df[\"discourse_type\"] = df[\"discourse_type\"].map(disc_types_mapping)\n",
    "\n",
    "df[\"paragraph_cnt\"] = df.essay_text.map(lambda x: len(x.split(\"\\n\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda49a0",
   "metadata": {
    "papermill": {
     "duration": 0.225768,
     "end_time": "2022-08-22T20:43:56.270639",
     "exception": false,
     "start_time": "2022-08-22T20:43:56.044871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "lgb_stacker_preds = []\n",
    "\n",
    "for fold in range(5):\n",
    "    print(fold)\n",
    "    gbm = lightgbm.Booster(model_file=f\"../input/lightgbm162/model_fold_{fold}.txt\")\n",
    "    valid_pred = gbm.predict(df[['discourse_type', 'Adequate', 'Effective', 'Ineffective', 'n_types',\n",
    "       'Ineffective_bin_0', 'Ineffective_bin_1', 'Ineffective_bin_2',\n",
    "       'mean_Ineffective', 'len_disc', 'paragraph_cnt']])\n",
    "    lgb_stacker_preds.append(valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fd0fa",
   "metadata": {
    "papermill": {
     "duration": 0.079664,
     "end_time": "2022-08-22T20:43:56.424963",
     "exception": false,
     "start_time": "2022-08-22T20:43:56.345299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb_stacker_preds = np.array(lgb_stacker_preds).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce36c7b",
   "metadata": {
    "papermill": {
     "duration": 1.100328,
     "end_time": "2022-08-22T20:43:57.596220",
     "exception": false,
     "start_time": "2022-08-22T20:43:56.495892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_groups = []\n",
    "\n",
    "gb = df_ind_models.groupby('essay_id', sort=False)\n",
    "for name, group in tqdm(gb):\n",
    "    group[\"n_types\"] = group.discourse_type.nunique()\n",
    "\n",
    "    for class_name in [\"Adequate\", \"Effective\", \"Ineffective\"]:\n",
    "        if class_name in [\"Adequate\", \"Effective\"]:\n",
    "            continue\n",
    "\n",
    "        for idx, val in enumerate(gen_x(group[class_name].values)):\n",
    "            group[f\"{class_name}_bin_{idx}\"] = val\n",
    "\n",
    "        group[f\"mean_{class_name}\"] = group[class_name].mean()\n",
    "        group[f\"max_{class_name}\"] = group[class_name].max()\n",
    "\n",
    "    for class_name in [f\"Ineffective_{i}\" for i in range(17)]:\n",
    "        group[f\"mean_{class_name}\"] = group[class_name].mean()\n",
    "\n",
    "    for class_name in [f\"Effective_{i}\" for i in range(17)]:\n",
    "        group[f\"mean_{class_name}\"] = group[class_name].mean()\n",
    "\n",
    "    all_groups.append(group)\n",
    "\n",
    "df_ind_models = pd.concat(all_groups).reset_index(drop=True)\n",
    "\n",
    "df_ind_models[\"paragraph_cnt\"] = df_ind_models.essay_text.map(lambda x: len(x.split(\"\\n\\n\")))\n",
    "\n",
    "disc_types_mapping = {'Lead': 0,\n",
    "'Position': 1,\n",
    "'Claim': 2,\n",
    "'Evidence': 3,\n",
    "'Counterclaim': 4,\n",
    "'Rebuttal': 5,\n",
    "'Concluding Statement': 6}\n",
    "\n",
    "df_ind_models[\"discourse_type\"] = df_ind_models[\"discourse_type\"].map(disc_types_mapping)\n",
    "\n",
    "for i in range(17):\n",
    "    df_ind_models = df_ind_models.drop([f'Adequate_{i}'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa8d4d",
   "metadata": {
    "papermill": {
     "duration": 0.213973,
     "end_time": "2022-08-22T20:43:57.879095",
     "exception": false,
     "start_time": "2022-08-22T20:43:57.665122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "lgb_stacker_preds_2 = []\n",
    "\n",
    "for fold in range(5):\n",
    "    print(fold)\n",
    "    gbm = lightgbm.Booster(model_file=f\"../input/lightgbm162v2/model_fold_{fold}.txt\")\n",
    "    valid_pred = gbm.predict(df_ind_models[['discourse_type', 'Adequate', 'Effective', 'Ineffective', 'Effective_0',\n",
    "       'Ineffective_0', 'Effective_1', 'Ineffective_1', 'Effective_2',\n",
    "       'Ineffective_2', 'Effective_3', 'Ineffective_3', 'Effective_4',\n",
    "       'Ineffective_4', 'Effective_5', 'Ineffective_5', 'Effective_6',\n",
    "       'Ineffective_6', 'Effective_7', 'Ineffective_7', 'Effective_8',\n",
    "       'Ineffective_8', 'Effective_9', 'Ineffective_9', 'Effective_10',\n",
    "       'Ineffective_10', 'Effective_11', 'Ineffective_11', 'Effective_12',\n",
    "       'Ineffective_12', 'Effective_13', 'Ineffective_13', 'Effective_14',\n",
    "       'Ineffective_14', 'Effective_15', 'Ineffective_15', 'Effective_16',\n",
    "       'Ineffective_16', 'n_types', 'Ineffective_bin_0', 'Ineffective_bin_1',\n",
    "       'Ineffective_bin_2', 'mean_Ineffective', 'max_Ineffective',\n",
    "       'mean_Ineffective_0', 'mean_Ineffective_1', 'mean_Ineffective_2',\n",
    "       'mean_Ineffective_3', 'mean_Ineffective_4', 'mean_Ineffective_5',\n",
    "       'mean_Ineffective_6', 'mean_Ineffective_7', 'mean_Ineffective_8',\n",
    "       'mean_Ineffective_9', 'mean_Ineffective_10', 'mean_Ineffective_11',\n",
    "       'mean_Ineffective_12', 'mean_Ineffective_13', 'mean_Ineffective_14',\n",
    "       'mean_Ineffective_15', 'mean_Ineffective_16', 'mean_Effective_0',\n",
    "       'mean_Effective_1', 'mean_Effective_2', 'mean_Effective_3',\n",
    "       'mean_Effective_4', 'mean_Effective_5', 'mean_Effective_6',\n",
    "       'mean_Effective_7', 'mean_Effective_8', 'mean_Effective_9',\n",
    "       'mean_Effective_10', 'mean_Effective_11', 'mean_Effective_12',\n",
    "       'mean_Effective_13', 'mean_Effective_14', 'mean_Effective_15',\n",
    "       'mean_Effective_16', 'paragraph_cnt']])\n",
    "    lgb_stacker_preds_2.append(valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c52c8",
   "metadata": {
    "papermill": {
     "duration": 0.074716,
     "end_time": "2022-08-22T20:43:58.022345",
     "exception": false,
     "start_time": "2022-08-22T20:43:57.947629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb_stacker_preds_2 = np.array(lgb_stacker_preds_2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae904a34",
   "metadata": {
    "papermill": {
     "duration": 0.079816,
     "end_time": "2022-08-22T20:43:58.459532",
     "exception": false,
     "start_time": "2022-08-22T20:43:58.379716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_preds = [\n",
    "    orig_preds.copy(),\n",
    "    lgb_stacker_preds.copy(),\n",
    "    nn_stacker_preds_1.copy(),\n",
    "    nn_stacker_preds_2.copy(),\n",
    "    lgb_stacker_preds_2.copy(),\n",
    "]\n",
    "\n",
    "all_preds = np.average(all_preds, axis=0, weights=[2.05643773, 1.02117978, 1.35595902, 0.2450152, 0.67321285])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f17fa",
   "metadata": {
    "papermill": {
     "duration": 0.080161,
     "end_time": "2022-08-22T20:43:59.061531",
     "exception": false,
     "start_time": "2022-08-22T20:43:58.981370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Adequate\"] = all_preds[:, 0]\n",
    "df[\"Effective\"] = all_preds[:, 1]\n",
    "df[\"Ineffective\"] = all_preds[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ccc87",
   "metadata": {
    "papermill": {
     "duration": 0.088352,
     "end_time": "2022-08-22T20:43:59.222398",
     "exception": false,
     "start_time": "2022-08-22T20:43:59.134046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['discourse_id', 'Ineffective', 'Adequate', 'Effective']].to_csv(\"pseudo_162_ff_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8aa1f",
   "metadata": {
    "papermill": {
     "duration": 0.090214,
     "end_time": "2022-08-22T20:43:59.383142",
     "exception": false,
     "start_time": "2022-08-22T20:43:59.292928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CALC_SCORE:\n",
    "    from sklearn.metrics import log_loss\n",
    "    \n",
    "    label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n",
    "    \n",
    "    y = np.zeros_like(all_preds)\n",
    "    \n",
    "    for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n",
    "        y[ii,jj] = 1\n",
    "        \n",
    "    print(log_loss(y, df[label_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9691c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1756.303876,
   "end_time": "2022-08-22T20:44:02.957273",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-22T20:14:46.653397",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
