{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209bfa1-9a2e-431d-a4fc-f99ae902b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a962e-ec42-4e77-bb9b-be187f451171",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../data/first_lvl_ensemble.pkl\", \"rb\") as f:\n",
    "    preds = pickle.load(f)\n",
    "    \n",
    "pp = np.load(\"../../data/first_lvl_ensemble.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9946c-2fee-49f1-acaa-89c735db8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/train_folded.csv\")\n",
    "label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n",
    "\n",
    "y = np.zeros((len(df),3))\n",
    "\n",
    "for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n",
    "    y[ii,jj] = 1\n",
    "\n",
    "for i,l in enumerate(label_cols):\n",
    "    df[l] = y[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5d819-befa-44ad-8055-dcc96e6a737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "class FeedbackStackerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, mode):\n",
    "        self.df = df.copy()\n",
    "        self.mode = mode\n",
    "\n",
    "        self.label_cols = label_cols.copy()\n",
    "        \n",
    "        p = [p[self.df.index.values] for p in preds.copy()]\n",
    "        p = np.stack(p)\n",
    "        \n",
    "        df = self.df\n",
    "        \n",
    "        X = []\n",
    "        for j in range(p.shape[0]):\n",
    "            cols = []\n",
    "            for jj, l in enumerate(label_cols):\n",
    "\n",
    "                df[f\"oof_{l}\"] = p[j,:,jj]\n",
    "                cols.append(f\"oof_{l}\")\n",
    "                \n",
    "                df[f\"oof_{l}_mean\"] = df.groupby(\"essay_id\")[f\"oof_{l}\"].transform(\"mean\")\n",
    "                cols.append(f\"oof_{l}_mean\")\n",
    "\n",
    "                df[f\"oof_{l}_t_mean\"] = df.groupby([\"essay_id\", \"discourse_type\"])[f\"oof_{l}\"].transform(\"mean\")\n",
    "                cols.append(f\"oof_{l}_t_mean\")\n",
    "                \n",
    "            df[f\"len\"] = df.groupby(\"essay_id\")[f\"discourse_id\"].transform(\"count\") / 10\n",
    "            cols.append(f\"len\")\n",
    "        \n",
    "            \n",
    "            X.append(df[cols].values)\n",
    "         \n",
    "        X = np.stack(X).transpose(1,2,0)\n",
    "        print(X.shape)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = self.df[self.label_cols].values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "ds = FeedbackStackerDataset(df.copy(), mode=\"train\")\n",
    "print(ds[0][0].shape)\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5258b92-4ce2-4679-a645-5582e590202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedbackStackerModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(FeedbackStackerModel, self).__init__()\n",
    "        \n",
    "        self.sizes = [256, 128, 64]\n",
    "        \n",
    "        layers = []\n",
    "        for j,s in enumerate(self.sizes):\n",
    "            if j == 0:\n",
    "                layers.append(nn.Conv1d(n_features, s, 1))\n",
    "            else:\n",
    "                layers.append(nn.Conv1d(self.sizes[j-1], s, 1))\n",
    "            layers.append(nn.PReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.head = nn.Linear(self.sizes[-1], 3)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = self.head(x)        \n",
    "        \n",
    "        output = {}\n",
    "        \n",
    "        output[\"logits\"] = x\n",
    "        \n",
    "        if self.training:\n",
    "            output[\"loss\"] = self.loss_fn(x, y.argmax(dim=1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc78cf-8994-4b1f-84dc-036e554d5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "\n",
    "exp_name = \"nn_v11_blend151_ff\"\n",
    "\n",
    "if not os.path.exists(f\"nn_models/{exp_name}\"):\n",
    "    os.makedirs(f\"nn_models/{exp_name}\")\n",
    "    \n",
    "oof = np.zeros_like(y)\n",
    "\n",
    "for FOLD in range(5):\n",
    "    \n",
    "    print()\n",
    "    print(f\"======FOLD {FOLD}=====\")\n",
    "    print()\n",
    "    df_train = df[df.fold!=FOLD]\n",
    "    df_val = df[df.fold==FOLD]\n",
    "\n",
    "    seed_preds = []\n",
    "    for seed in range(1):\n",
    "\n",
    "        DEVICE = \"cuda:0\"\n",
    "\n",
    "        train_ds = FeedbackStackerDataset(df, mode=\"train\")\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n",
    "                                      pin_memory=False, drop_last=True)\n",
    "\n",
    "        val_ds = FeedbackStackerDataset(df_val, mode=\"valid\")\n",
    "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
    "                                      pin_memory=False, drop_last=False)\n",
    "\n",
    "        model = FeedbackStackerModel(n_features=train_ds.X.shape[1])\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr = LR)\n",
    "\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * len(train_loader))\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for e in tqdm(range(EPOCHS)):\n",
    "            #print(f\"-----EPOCH {e} -----\")\n",
    "            tbar = tqdm(train_loader, disable=True)\n",
    "\n",
    "\n",
    "            loss_list = []\n",
    "            p = []\n",
    "            y_train = []\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for idx, data in enumerate(tbar):\n",
    "                data = [x.to(DEVICE) for x in data]\n",
    "                inputs, target = data\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs, target)\n",
    "\n",
    "                loss = output[\"loss\"]\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "                avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "                tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                p.append(output[\"logits\"].softmax(dim=1).detach().cpu().numpy())\n",
    "                y_train.append(target.detach().cpu().numpy())\n",
    "\n",
    "            y_train = np.concatenate(y_train, axis=0)\n",
    "            p = np.concatenate(p, axis=0)\n",
    "            err = log_loss(y_train.argmax(axis=1), p)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            p = []\n",
    "            for idx, data in enumerate(val_loader):\n",
    "                data = [x.to(DEVICE) for x in data]\n",
    "                inputs, target = data\n",
    "\n",
    "                output = model(inputs, target)\n",
    "\n",
    "                p.append(output[\"logits\"].softmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "            y_val = y[df_val.index.values]\n",
    "            p = np.concatenate(p, axis=0)\n",
    "            err = log_loss(y_val.argmax(axis=1), p)\n",
    "\n",
    "        print(\"-----\")\n",
    "        seed_preds.append(p.copy())\n",
    "        p = np.mean(seed_preds, axis=0)\n",
    "        \n",
    "        oof[df_val.index.values] = p\n",
    "        \n",
    "        err = log_loss(y[df_val.index.values].argmax(axis=1), p)\n",
    "        print(\"VAL SEED BLEND\", err)\n",
    "\n",
    "        ppp = 0.5*p.copy() + 0.5*pp[df_val.index.values]\n",
    "        for _ in range(10):\n",
    "\n",
    "            ppp = ppp * (y.mean(axis=0) / ppp.mean(axis=0)) #* (y.std(axis=0)/ppp.std(axis=0))\n",
    "\n",
    "\n",
    "            ppp = ppp / ppp.sum(axis=1, keepdims=True)\n",
    "\n",
    "        err = log_loss(y[df_val.index.values].argmax(axis=1), ppp)\n",
    "        print(\"VAL SEED BLEND OPT\", err)\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"nn_models/{exp_name}/checkpoint_fold{FOLD}_seed{seed}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39c30b-38aa-4e7a-b3dc-5e2e194b14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df.copy()\n",
    "df_p[label_cols] = oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5666a-c3d4-4e47-b0cc-f8ecea3eca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p[[\"discourse_id\"]+label_cols].to_csv(f\"nn_models/{exp_name}_validation_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9765bb5-d04b-4071-afe6-6aa4a840f3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
